# -*- coding: utf-8 -*-
"""Context_Assesment_Of_Indian_Road.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EDo3VN3rB3pdTEQsyB8EQNeoCtd-333t
"""

import os
import cv2
import numpy as np
import textwrap
import torch
from torchvision import models, transforms
from PIL import Image
from google.colab import drive
import matplotlib.pyplot as plt

# Mount Google Drive to access your dataset
drive.mount('/content/drive')

# Path to the folder containing images in Google Drive
image_folder = "/content/drive/My Drive/indian_vehicle_images"

# Path to the folder containing text annotations in Google Drive
caption_folder = "/content/drive/My Drive/indian_vehicle_txt_annos"

# Get list of image files
image_files = os.listdir(image_folder)

# Load the pre-trained DeepLabV3 model
model = models.segmentation.deeplabv3_resnet101(pretrained=True).eval()

# Image transformation pipeline
preprocess = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Function to display caption on a white background
def create_caption_image(caption, width):
    # Wrap caption text
    wrapped_caption = textwrap.fill(caption, width=70)

    # Calculate the size of the white background image
    font_scale = 0.8
    font = cv2.FONT_HERSHEY_SIMPLEX
    lines = textwrap.wrap(wrapped_caption, width=70)
    text_height = 30 * len(lines)
    margin = 10
    max_line_width = max(cv2.getTextSize(line, font, font_scale, 2)[0][0] for line in lines)
    caption_img = np.ones((text_height + 2*margin, max_line_width + 2*margin, 3), dtype=np.uint8) * 255

    # Put wrapped caption text on the image
    y = margin + 30  # Start from a small margin
    for line in lines:
        cv2.putText(caption_img, line, (margin, y), font, font_scale, (0, 0, 0), 2)
        y += 30

    # Resize caption image to match the width of the original image
    caption_img = cv2.resize(caption_img, (width, caption_img.shape[0]))

    return caption_img

# Function to apply segmentation on image
def segment_image(image_path):
    # Load image
    input_image = Image.open(image_path).convert("RGB")
    input_tensor = preprocess(input_image)
    input_batch = input_tensor.unsqueeze(0)

    with torch.no_grad():
        output = model(input_batch)['out'][0]
    output_predictions = output.argmax(0).byte().cpu().numpy()

    return output_predictions

# Function to display image
def display_image_with_caption_and_segmentation(image_path, caption_img, segmentation_img):
    img = cv2.imread(image_path)
    # Resize image to a larger size
    img = cv2.resize(img, (800, 600))

    # Create color map for segmentation
    colormap = np.zeros((256, 3), dtype=np.uint8)
    colormap[1] = [0, 255, 0]  # Example color mapping for class 1

    # Apply color map to segmentation image
    segmentation_img_color = colormap[segmentation_img]

    # Resize segmentation image to match the resized original image
    segmentation_img_color = cv2.resize(segmentation_img_color, (800, 600))

    # Calculate the total height of the combined image
    total_height = img.shape[0] + segmentation_img_color.shape[0] + caption_img.shape[0]

    # Create a blank white image to hold the combined image
    combined_img = np.ones((total_height, img.shape[1], 3), dtype=np.uint8) * 255

    # Copy the image, segmentation image, and the caption image into the combined image
    combined_img[:img.shape[0], :, :] = img
    combined_img[img.shape[0]:img.shape[0]+segmentation_img_color.shape[0], :, :] = segmentation_img_color
    combined_img[img.shape[0]+segmentation_img_color.shape[0]:, :, :] = caption_img

    # Display the combined image
    plt.figure(figsize=(10, 10))
    plt.imshow(cv2.cvtColor(combined_img, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.show()

# Randomly select an image
random_image_file = np.random.choice(image_files)
# Load image
image_path = os.path.join(image_folder, random_image_file)
print("Image Path:", image_path)

# Load corresponding caption
image_name = os.path.splitext(random_image_file)[0]
caption_file = image_name + ".txt"
caption_path = os.path.join(caption_folder, caption_file)
print("Caption Path:", caption_path)

# Read caption text
with open(caption_path, 'r', encoding='utf-8') as f:
    caption = f.read().strip()

# Create caption image
caption_img = create_caption_image(caption, 800)

# Apply segmentation to the image
segmentation_img = segment_image(image_path)

# Display image with caption and segmentation
display_image_with_caption_and_segmentation(image_path, caption_img, segmentation_img)

import os
import cv2
import numpy as np
import textwrap
import torch
from torchvision import models, transforms
from PIL import Image
import matplotlib.pyplot as plt

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to the folders containing images and captions
image_folder = "/content/drive/My Drive/indian_vehicle_images"
caption_folder = "/content/drive/My Drive/indian_vehicle_txt_annos"

# Check if paths are correct
print("Images Path Exists:", os.path.exists(image_folder))
print("Captions Path Exists:", os.path.exists(caption_folder))

# Get list of image files
image_files = os.listdir(image_folder)

# Load the pre-trained DeepLabV3 model for segmentation
print("Loading segmentation model...")
segmentation_model = models.segmentation.deeplabv3_resnet101(pretrained=True).eval()
print("Segmentation model loaded.")

# Image transformation pipeline for segmentation
preprocess = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Function to apply segmentation on image
def segment_image(image_path):
    input_image = Image.open(image_path).convert("RGB")
    input_tensor = preprocess(input_image)
    input_batch = input_tensor.unsqueeze(0)
    print("Running segmentation model...")
    with torch.no_grad():
        output = segmentation_model(input_batch)['out'][0]
    output_predictions = output.argmax(0).byte().cpu().numpy()
    print("Segmentation complete.")
    return output_predictions

# Function to display image with segmentation and caption
def display_image_with_segmentation_and_caption(image_path, segmentation_img, caption):
    img = cv2.imread(image_path)
    img = cv2.resize(img, (800, 600))

    # Print min and max values of segmentation output
    print("Min value of segmentation output:", np.min(segmentation_img))
    print("Max value of segmentation output:", np.max(segmentation_img))

    # Apply segmentation overlay
    segmentation_img_color = cv2.applyColorMap((segmentation_img * 255).astype(np.uint8), cv2.COLORMAP_JET)
    segmentation_img_color = cv2.resize(segmentation_img_color, (800, 600))

    # Display the segmentation overlay
    plt.figure(figsize=(10, 10))
    plt.imshow(segmentation_img_color)
    plt.axis('off')
    plt.title('Segmentation Overlay')
    plt.show()

    # Display the caption
    print("Caption:", caption)

    # Combine images
    total_height = img.shape[0] + segmentation_img_color.shape[0]
    combined_img = np.ones((total_height, img.shape[1], 3), dtype=np.uint8) * 255
    combined_img[:img.shape[0], :, :] = img
    combined_img[img.shape[0]:, :, :] = segmentation_img_color

    # Display the combined image
    plt.figure(figsize=(10, 10))
    plt.imshow(cv2.cvtColor(combined_img, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.show()

# Randomly select an image
random_image_file = np.random.choice(image_files)
image_path = os.path.join(image_folder, random_image_file)
print("Image Path:", image_path)

# Apply segmentation to the image
segmentation_img = segment_image(image_path)

# Get the corresponding caption
image_name = os.path.splitext(random_image_file)[0]
caption_file = image_name + ".txt"
caption_path = os.path.join(caption_folder, caption_file)

with open(caption_path, 'r', encoding='utf-8') as f:
    caption = f.read().strip()

# Display image with segmentation and caption
display_image_with_segmentation_and_caption(image_path, segmentation_img, caption)

import os
import cv2
import numpy as np
import textwrap
import torch
from torchvision import models, transforms
from PIL import Image
import matplotlib.pyplot as plt
import torchvision.transforms.functional as F

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to the folders containing images and captions
image_folder = "/content/drive/My Drive/indian_vehicle_images"
caption_folder = "/content/drive/My Drive/indian_vehicle_txt_annos"

# Check if paths are correct
print("Images Path Exists:", os.path.exists(image_folder))
print("Captions Path Exists:", os.path.exists(caption_folder))

# Get list of image files
image_files = os.listdir(image_folder)

# Load the pre-trained DeepLabV3 model for segmentation
print("Loading segmentation model...")
segmentation_model = models.segmentation.deeplabv3_resnet101(pretrained=True).eval()
print("Segmentation model loaded.")

# Load the pre-trained Faster R-CNN model for object detection
print("Loading object detection model...")
object_detection_model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True).eval()
print("Object detection model loaded.")

# Image transformation pipeline for segmentation
segmentation_preprocess = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Image transformation pipeline for object detection
detection_preprocess = transforms.Compose([
    transforms.Resize((800, 800)),
    transforms.ToTensor()
])

# Function to apply semantic segmentation on image
def segment_image(image_path):
    input_image = Image.open(image_path).convert("RGB")
    input_tensor = segmentation_preprocess(input_image)
    input_batch = input_tensor.unsqueeze(0)
    with torch.no_grad():
        output = segmentation_model(input_batch)['out'][0]
    output_predictions = output.argmax(0).byte().cpu().numpy()
    return output_predictions

# Function to apply object detection on image
def detect_objects(image_path):
    input_image = Image.open(image_path).convert("RGB")
    input_tensor = detection_preprocess(input_image)
    input_batch = input_tensor.unsqueeze(0)
    with torch.no_grad():
        output = object_detection_model(input_batch)
    return output[0]

# Function to display image with segmentation, object detection, and caption
def display_image_with_detection_and_caption(image_path, segmentation_img, objects, caption):
    img = cv2.imread(image_path)
    img = cv2.resize(img, (800, 600))

    # Apply segmentation overlay with specific colors
    segmentation_img_color = np.zeros((segmentation_img.shape[0], segmentation_img.shape[1], 3), dtype=np.uint8)
    segmentation_img_color[segmentation_img == 0] = [0, 0, 0]  # Background (black)
    segmentation_img_color[segmentation_img == 1] = [255, 255, 255]  # Road (white)
    segmentation_img_color[segmentation_img == 2] = [0, 0, 255]  # Vehicles (red)

    # Display the input image
    plt.figure(figsize=(10, 10))
    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.title('Input Image')
    plt.show()

    # Display the segmentation overlay
    plt.figure(figsize=(10, 10))
    plt.imshow(segmentation_img_color)
    plt.axis('off')
    plt.title('Semantic Segmentation Overlay')
    plt.show()

    # Apply bounding boxes for object detection
    for box in objects['boxes']:
        cv2.rectangle(img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)

    # Display the image with bounding boxes
    plt.figure(figsize=(10, 10))
    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.title('Object Detection with Bounding Boxes')
    plt.show()

    # Display the caption
    print("Caption:", caption)

# Randomly select an image
random_image_file = np.random.choice(image_files)
image_path = os.path.join(image_folder, random_image_file)
print("Image Path:", image_path)

# Apply semantic segmentation to the image
segmentation_img = segment_image(image_path)

# Apply object detection to the image
objects = detect_objects(image_path)

# Get the corresponding caption
image_name = os.path.splitext(random_image_file)[0]
caption_file = image_name + ".txt"
caption_path = os.path.join(caption_folder, caption_file)

with open(caption_path, 'r', encoding='utf-8') as f:
    caption = f.read().strip()

# Display image with segmentation, object detection, and caption
display_image_with_detection_and_caption(image_path, segmentation_img, objects, caption)

# Install detectron2
!pip install torch torchvision
!pip install 'git+https://github.com/facebookresearch/detectron2.git'

# Import necessary libraries
import os
import cv2
import numpy as np
import textwrap
import torch
import torchvision.transforms as T
from PIL import Image
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
from detectron2.structures import Instances
from detectron2.utils.visualizer import Visualizer

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to the folders containing images and captions
image_folder = "/content/drive/My Drive/indian_vehicle_images"
caption_folder = "/content/drive/My Drive/indian_vehicle_txt_annos"

# Check if paths are correct
print("Images Path Exists:", os.path.exists(image_folder))
print("Captions Path Exists:", os.path.exists(caption_folder))

# Load the pre-trained Mask R-CNN model
print("Loading Mask R-CNN model...")
cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
predictor = DefaultPredictor(cfg)
print("Mask R-CNN model loaded.")

# Function to apply object detection and segmentation on image
def detect_and_segment(image_path):
    im = cv2.imread(image_path)
    outputs = predictor(im)
    v = Visualizer(im[:, :, ::-1], scale=1.2)
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    return out.get_image()[:, :, ::-1]

# Randomly select an image
random_image_file = np.random.choice(os.listdir(image_folder))
image_path = os.path.join(image_folder, random_image_file)
print("Image Path:", image_path)

# Apply object detection and segmentation to the image
output_image = detect_and_segment(image_path)

# Display the output image
plt.figure(figsize=(10, 10))
plt.imshow(output_image)
plt.axis('off')
plt.title('Object Detection and Segmentation Output')
plt.show()

import os
import cv2
import numpy as np
import torch
import torchvision.transforms as T
from PIL import Image
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
from detectron2.utils.visualizer import Visualizer
import matplotlib.pyplot as plt

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to the folders containing images and captions
image_folder = "/content/drive/My Drive/indian_vehicle_images"
caption_folder = "/content/drive/My Drive/indian_vehicle_txt_annos"

# Check if paths are correct
print("Images Path Exists:", os.path.exists(image_folder))
print("Captions Path Exists:", os.path.exists(caption_folder))

# Load the pre-trained Mask R-CNN model for object detection
print("Loading Mask R-CNN model for object detection...")
cfg_object_detection = get_cfg()
cfg_object_detection.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg_object_detection.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_object_detection.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
predictor_object_detection = DefaultPredictor(cfg_object_detection)
print("Mask R-CNN model for object detection loaded.")

# Load the pre-trained Mask R-CNN model for semantic segmentation
print("Loading Mask R-CNN model for semantic segmentation...")
cfg_semantic_segmentation = get_cfg()
cfg_semantic_segmentation.merge_from_file(model_zoo.get_config_file("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml"))
cfg_semantic_segmentation.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_semantic_segmentation.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml")
predictor_semantic_segmentation = DefaultPredictor(cfg_semantic_segmentation)
print("Mask R-CNN model for semantic segmentation loaded.")

# Function to apply object detection on image
def detect_objects(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_object_detection(im)
    v = Visualizer(im[:, :, ::-1], scale=1.2)
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    return out.get_image()[:, :, ::-1]

# Function to apply semantic segmentation on image
def segment_image(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_semantic_segmentation(im)
    panoptic_seg, segments_info = outputs["panoptic_seg"]
    v = Visualizer(im[:, :, ::-1], scale=1.2)
    out = v.draw_panoptic_seg_predictions(panoptic_seg.to("cpu"), segments_info)
    return out.get_image()[:, :, ::-1]

# Randomly select an image
random_image_file = np.random.choice(os.listdir(image_folder))
image_path = os.path.join(image_folder, random_image_file)
print("Image Path:", image_path)

# Apply object detection and semantic segmentation to the image
output_object_detection = detect_objects(image_path)
output_semantic_segmentation = segment_image(image_path)

# Display the input image, object detection, and semantic segmentation results
fig, axs = plt.subplots(1, 3, figsize=(15, 5))
input_image = cv2.imread(image_path)[:, :, ::-1]
axs[0].imshow(input_image)
axs[0].set_title('Input Image')
axs[0].axis('off')
axs[1].imshow(output_object_detection)
axs[1].set_title('Object Detection')
axs[1].axis('off')
axs[2].imshow(output_semantic_segmentation)
axs[2].set_title('Semantic Segmentation')
axs[2].axis('off')
plt.show()

# Install detectron2
!pip install torch torchvision
!pip install 'git+https://github.com/facebookresearch/detectron2.git'

# You may need to restart the runtime after installing detectron2

# Install detectron2
!pip install torch torchvision
!pip install 'git+https://github.com/facebookresearch/detectron2.git'

# You may need to restart the runtime after installing detectron2

import os
import cv2
import numpy as np
import torch
import torchvision.transforms as T
from PIL import Image
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
from detectron2.utils.visualizer import Visualizer
import matplotlib.pyplot as plt

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to the folders containing images and captions
image_folder = "/content/drive/My Drive/indian_vehicle_images"
caption_folder = "/content/drive/My Drive/indian_vehicle_txt_annos"

# Load the pre-trained Mask R-CNN model for object detection
cfg_object_detection = get_cfg()
cfg_object_detection.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg_object_detection.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5
cfg_object_detection.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
predictor_object_detection = DefaultPredictor(cfg_object_detection)

# Load the pre-trained Mask R-CNN model for semantic segmentation
cfg_semantic_segmentation = get_cfg()
cfg_semantic_segmentation.merge_from_file(model_zoo.get_config_file("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml"))
cfg_semantic_segmentation.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5
cfg_semantic_segmentation.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml")
predictor_semantic_segmentation = DefaultPredictor(cfg_semantic_segmentation)

# Function to apply object detection on image
def detect_objects(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_object_detection(im)
    v = Visualizer(im[:, :, ::-1], scale=1.2)
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    return out.get_image()[:, :, ::-1]

# Function to apply semantic segmentation on image
def segment_image(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_semantic_segmentation(im)
    v = Visualizer(im[:, :, ::-1], scale=1.2)
    out = v.draw_sem_seg(outputs["sem_seg"].argmax(dim=0).to("cpu"))
    return out.get_image()[:, :, ::-1]

# Randomly select an image
random_image_file = np.random.choice(os.listdir(image_folder))
image_path = os.path.join(image_folder, random_image_file)
print("Image Path:", image_path)

# Apply object detection and semantic segmentation to the image
output_object_detection = detect_objects(image_path)
output_semantic_segmentation = segment_image(image_path)

# Display the input image, object detection, and semantic segmentation results
fig, axs = plt.subplots(1, 3, figsize=(15, 5))
input_image = cv2.imread(image_path)[:, :, ::-1]
axs[0].imshow(input_image)
axs[0].set_title('Input Image')
axs[0].axis('off')
axs[1].imshow(output_object_detection)
axs[1].set_title('Object Detection')
axs[1].axis('off')
axs[2].imshow(output_semantic_segmentation)
axs[2].set_title('Semantic Segmentation')
axs[2].axis('off')
plt.show()

import os
import cv2
import numpy as np
import torch
import torchvision.transforms as T
from PIL import Image
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
from detectron2.utils.visualizer import Visualizer
import matplotlib.pyplot as plt

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to the folders containing images and captions
image_folder = "/content/drive/My Drive/indian_vehicle_images"
caption_folder = "/content/drive/My Drive/indian_vehicle_txt_annos"

# Check if paths are correct
print("Images Path Exists:", os.path.exists(image_folder))
print("Captions Path Exists:", os.path.exists(caption_folder))

# Load the pre-trained Mask R-CNN model for object detection
print("Loading Mask R-CNN model for object detection...")
cfg_object_detection = get_cfg()
cfg_object_detection.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg_object_detection.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_object_detection.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
predictor_object_detection = DefaultPredictor(cfg_object_detection)
print("Mask R-CNN model for object detection loaded.")

# Load the pre-trained Mask R-CNN model for semantic segmentation
print("Loading Mask R-CNN model for semantic segmentation...")
cfg_semantic_segmentation = get_cfg()
cfg_semantic_segmentation.merge_from_file(model_zoo.get_config_file("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml"))
cfg_semantic_segmentation.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_semantic_segmentation.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml")
predictor_semantic_segmentation = DefaultPredictor(cfg_semantic_segmentation)
print("Mask R-CNN model for semantic segmentation loaded.")

# Function to apply object detection on image
def detect_objects(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_object_detection(im)
    v = Visualizer(im[:, :, ::-1], scale=1.2)
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    return out.get_image()[:, :, ::-1]

# Function to apply semantic segmentation on image
def segment_image(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_semantic_segmentation(im)
    v = Visualizer(im[:, :, ::-1], metadata=predictor_semantic_segmentation.metadata, scale=1.2)
    out = v.draw_sem_seg(outputs["sem_seg"].argmax(dim=0).to("cpu"))
    return out.get_image()[:, :, ::-1]

# Randomly select an image
random_image_file = np.random.choice(os.listdir(image_folder))
image_path = os.path.join(image_folder, random_image_file)
print("Image Path:", image_path)

# Apply object detection and semantic segmentation to the image
output_object_detection = detect_objects(image_path)
output_semantic_segmentation = segment_image(image_path)

# Display the input image, object detection, and semantic segmentation results
fig, axs = plt.subplots(1, 3, figsize=(15, 5))
input_image = cv2.imread(image_path)[:, :, ::-1]
axs[0].imshow(input_image)
axs[0].set_title('Input Image')
axs[0].axis('off')
axs[1].imshow(output_object_detection)
axs[1].set_title('Object Detection')
axs[1].axis('off')
axs[2].imshow(output_semantic_segmentation)
axs[2].set_title('Semantic Segmentation')
axs[2].axis('off')
plt.show()

import os
import cv2
import numpy as np
import torch
import torchvision.transforms as T
from PIL import Image
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
from detectron2.utils.visualizer import Visualizer
import matplotlib.pyplot as plt

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to the folders containing images and captions
image_folder = "/content/drive/My Drive/indian_vehicle_images"
caption_folder = "/content/drive/My Drive/indian_vehicle_txt_annos"

# Check if paths are correct
print("Images Path Exists:", os.path.exists(image_folder))
print("Captions Path Exists:", os.path.exists(caption_folder))

# Load the pre-trained Mask R-CNN model for object detection
print("Loading Mask R-CNN model for object detection...")
cfg_object_detection = get_cfg()
cfg_object_detection.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg_object_detection.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_object_detection.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
predictor_object_detection = DefaultPredictor(cfg_object_detection)
print("Mask R-CNN model for object detection loaded.")

# Load the pre-trained Mask R-CNN model for semantic segmentation
print("Loading Mask R-CNN model for semantic segmentation...")
cfg_semantic_segmentation = get_cfg()
cfg_semantic_segmentation.merge_from_file(model_zoo.get_config_file("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml"))
cfg_semantic_segmentation.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_semantic_segmentation.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml")
predictor_semantic_segmentation = DefaultPredictor(cfg_semantic_segmentation)
print("Mask R-CNN model for semantic segmentation loaded.")

# Function to apply object detection on image
def detect_objects(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_object_detection(im)
    v = Visualizer(im[:, :, ::-1], scale=1.2)
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    return out.get_image()[:, :, ::-1]

# Function to apply semantic segmentation on image
def segment_image(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_semantic_segmentation(im)
    v = Visualizer(im[:, :, ::-1], metadata=predictor_semantic_segmentation.metadata, scale=1.2)
    out = v.draw_sem_seg(outputs["sem_seg"].argmax(dim=0).to("cpu"))
    return out.get_image()[:, :, ::-1]

# Function to read caption from file
def read_caption(image_name):
    caption_file = os.path.join(caption_folder, f"{image_name}.txt")
    if os.path.exists(caption_file):
        with open(caption_file, 'r') as f:
            caption = f.read().strip()
        return caption
    else:
        return "No caption available."

# Randomly select an image
random_image_file = np.random.choice(os.listdir(image_folder))
image_path = os.path.join(image_folder, random_image_file)
image_name = os.path.splitext(random_image_file)[0]
print("Image Path:", image_path)

# Apply object detection and semantic segmentation to the image
output_object_detection = detect_objects(image_path)
output_semantic_segmentation = segment_image(image_path)

# Read the caption for the image
caption = read_caption(image_name)

# Display the input image, object detection, and semantic segmentation results along with the caption
fig, axs = plt.subplots(1, 3, figsize=(15, 5))
input_image = cv2.imread(image_path)[:, :, ::-1]
axs[0].imshow(input_image)
axs[0].set_title('Input Image')
axs[0].axis('off')
axs[1].imshow(output_object_detection)
axs[1].set_title('Object Detection')
axs[1].axis('off')
axs[2].imshow(output_semantic_segmentation)
axs[2].set_title('Semantic Segmentation')
axs[2].axis('off')

# Add caption below the images
plt.figtext(0.5, 0.01, caption, wrap=True, horizontalalignment='center', fontsize=12)

plt.show()



import os
import cv2
import numpy as np
import torch
import torchvision.transforms as T
from PIL import Image
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
from detectron2.utils.visualizer import Visualizer
import matplotlib.pyplot as plt

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to the folders containing images and captions
image_folder = "/content/drive/My Drive/indian_vehicle_images"

# Check if the path is correct
print("Images Path Exists:", os.path.exists(image_folder))

# Load the pre-trained Mask R-CNN model for object detection
print("Loading Mask R-CNN model for object detection...")
cfg_object_detection = get_cfg()
cfg_object_detection.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg_object_detection.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_object_detection.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
predictor_object_detection = DefaultPredictor(cfg_object_detection)
print("Mask R-CNN model for object detection loaded.")

# Load the pre-trained Mask R-CNN model for semantic segmentation
print("Loading Mask R-CNN model for semantic segmentation...")
cfg_semantic_segmentation = get_cfg()
cfg_semantic_segmentation.merge_from_file(model_zoo.get_config_file("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml"))
cfg_semantic_segmentation.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_semantic_segmentation.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml")
predictor_semantic_segmentation = DefaultPredictor(cfg_semantic_segmentation)
print("Mask R-CNN model for semantic segmentation loaded.")

# Function to apply object detection on image
def detect_objects(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_object_detection(im)
    return outputs

# Function to apply semantic segmentation on image
def segment_image(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_semantic_segmentation(im)
    return outputs

# Function to generate caption based on object detection and semantic segmentation results
def generate_caption(outputs_detection, outputs_segmentation):
    instances = outputs_detection["instances"]
    sem_seg = outputs_segmentation["sem_seg"]

    num_instances = len(instances)
    num_classes = sem_seg.shape[0]

    caption = ""

    # Add object detection information to the caption
    if num_instances > 0:
        caption += f"Detected {num_instances} objects: "
        for i in range(num_instances):
            class_id = instances.pred_classes[i].item()
            score = instances.scores[i].item()
            caption += f"{class_id} (Score: {score}), "

    # Add semantic segmentation information to the caption
    if num_classes > 0:
        caption += f"Segmented into {num_classes} classes: "
        for i in range(num_classes):
            class_id = i
            class_area = (sem_seg[i] > 0).sum().item()
            caption += f"{class_id} (Area: {class_area}), "

    return caption

# Randomly select an image
random_image_file = np.random.choice(os.listdir(image_folder))
image_path = os.path.join(image_folder, random_image_file)
print("Image Path:", image_path)

# Apply object detection and semantic segmentation to the image
outputs_detection = detect_objects(image_path)
outputs_segmentation = segment_image(image_path)

# Generate caption based on the detection and segmentation results
caption = generate_caption(outputs_detection, outputs_segmentation)
print("Generated Caption:", caption)

import os
import cv2
import numpy as np
import torch
import torchvision.transforms as T
from PIL import Image
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
from detectron2.utils.visualizer import Visualizer
import matplotlib.pyplot as plt

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to the folders containing images and captions
image_folder = "/content/drive/My Drive/indian_vehicle_images"
caption_folder = "/content/drive/My Drive/indian_vehicle_txt_annos"

# Check if paths are correct
print("Images Path Exists:", os.path.exists(image_folder))
print("Captions Path Exists:", os.path.exists(caption_folder))

# Load the pre-trained Mask R-CNN model for object detection
print("Loading Mask R-CNN model for object detection...")
cfg_object_detection = get_cfg()
cfg_object_detection.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg_object_detection.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_object_detection.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
predictor_object_detection = DefaultPredictor(cfg_object_detection)
print("Mask R-CNN model for object detection loaded.")

# Load the pre-trained Mask R-CNN model for semantic segmentation
print("Loading Mask R-CNN model for semantic segmentation...")
cfg_semantic_segmentation = get_cfg()
cfg_semantic_segmentation.merge_from_file(model_zoo.get_config_file("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml"))
cfg_semantic_segmentation.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_semantic_segmentation.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml")
predictor_semantic_segmentation = DefaultPredictor(cfg_semantic_segmentation)
print("Mask R-CNN model for semantic segmentation loaded.")

# Function to apply object detection on image
def detect_objects(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_object_detection(im)
    v = Visualizer(im[:, :, ::-1], scale=1.2)
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    return out.get_image()[:, :, ::-1]

# Function to apply semantic segmentation on image
def segment_image(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_semantic_segmentation(im)
    v = Visualizer(im[:, :, ::-1], metadata=predictor_semantic_segmentation.metadata, scale=1.2)
    out = v.draw_sem_seg(outputs["sem_seg"].argmax(dim=0).to("cpu"))
    return out.get_image()[:, :, ::-1]

# Function to read caption from file
def read_caption(image_name):
    caption_file = os.path.join(caption_folder, f"{image_name}.txt")
    if os.path.exists(caption_file):
        with open(caption_file, 'r') as f:
            caption = f.read().strip()
        return caption
    else:
        return "No caption available."

# Randomly select an image
random_image_file = np.random.choice(os.listdir(image_folder))
image_path = os.path.join(image_folder, random_image_file)
image_name = os.path.splitext(random_image_file)[0]
print("Image Path:", image_path)

# Apply object detection and semantic segmentation to the image
output_object_detection = detect_objects(image_path)
output_semantic_segmentation = segment_image(image_path)

# Read the caption for the image
caption = read_caption(image_name)

# Display the input image, object detection, and semantic segmentation results along with the caption
fig, axs = plt.subplots(1, 3, figsize=(15, 5))
input_image = cv2.imread(image_path)[:, :, ::-1]
axs[0].imshow(input_image)
axs[0].set_title('Input Image')
axs[0].axis('off')
axs[1].imshow(output_object_detection)
axs[1].set_title('Object Detection')
axs[1].axis('off')
axs[2].imshow(output_semantic_segmentation)
axs[2].set_title('Semantic Segmentation')
axs[2].axis('off')

# Add caption below the images
plt.figtext(0.5, 0.01, caption, wrap=True, horizontalalignment='center', fontsize=12)

plt.show()



import os
import cv2
import numpy as np
import torch
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, jaccard_score
from PIL import Image
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
from detectron2.utils.visualizer import Visualizer
from detectron2.data.detection_utils import read_image
import matplotlib.pyplot as plt

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to the folders containing images and captions
image_folder = "/content/drive/My Drive/indian_vehicle_images"
caption_folder = "/content/drive/My Drive/indian_vehicle_txt_annos"
annotation_folder = "/content/drive/My Drive/indian_vehicle_annotations"  # Folder containing ground truth annotations

# Check if paths are correct
print("Images Path Exists:", os.path.exists(image_folder))
print("Captions Path Exists:", os.path.exists(caption_folder))
print("Annotations Path Exists:", os.path.exists(annotation_folder))

# Load the pre-trained Mask R-CNN model for object detection
print("Loading Mask R-CNN model for object detection...")
cfg_object_detection = get_cfg()
cfg_object_detection.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg_object_detection.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_object_detection.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
predictor_object_detection = DefaultPredictor(cfg_object_detection)
print("Mask R-CNN model for object detection loaded.")

# Load the pre-trained Mask R-CNN model for semantic segmentation
print("Loading Mask R-CNN model for semantic segmentation...")
cfg_semantic_segmentation = get_cfg()
cfg_semantic_segmentation.merge_from_file(model_zoo.get_config_file("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml"))
cfg_semantic_segmentation.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_semantic_segmentation.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml")
predictor_semantic_segmentation = DefaultPredictor(cfg_semantic_segmentation)
print("Mask R-CNN model for semantic segmentation loaded.")

# Function to apply object detection on image
def detect_objects(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_object_detection(im)
    v = Visualizer(im[:, :, ::-1], scale=1.2)
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    return out.get_image()[:, :, ::-1], outputs

# Function to apply semantic segmentation on image
def segment_image(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_semantic_segmentation(im)
    v = Visualizer(im[:, :, ::-1], metadata=predictor_semantic_segmentation.metadata, scale=1.2)
    out = v.draw_sem_seg(outputs["sem_seg"].argmax(dim=0).to("cpu"))
    return out.get_image()[:, :, ::-1], outputs

# Function to read caption from file
def read_caption(image_name):
    caption_file = os.path.join(caption_folder, f"{image_name}.txt")
    if os.path.exists(caption_file):
        with open(caption_file, 'r') as f:
            caption = f.read().strip()
        return caption
    else:
        return "No caption available."

# Function to calculate object detection accuracy
def calculate_object_detection_accuracy(preds, gt):
    # Assuming gt and preds are lists of bounding boxes in [x1, y1, x2, y2] format
    iou_threshold = 0.5
    tp, fp, fn = 0, 0, 0

    for pred in preds:
        ious = [iou(pred, g) for g in gt]
        max_iou = max(ious) if ious else 0
        if max_iou >= iou_threshold:
            tp += 1
        else:
            fp += 1

    fn = len(gt) - tp

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    return precision, recall, f1

# Function to calculate IoU for bounding boxes
def iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])

    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)
    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)

    iou = interArea / float(boxAArea + boxBArea - interArea)
    return iou

# Function to calculate semantic segmentation accuracy
def calculate_semantic_segmentation_accuracy(pred, gt):
    pred = pred.argmax(dim=0).cpu().numpy()
    gt = gt.cpu().numpy()
    pixel_accuracy = np.mean(pred == gt)
    mean_iou = jaccard_score(gt.flatten(), pred.flatten(), average='macro')
    return pixel_accuracy, mean_iou

# Randomly select an image
random_image_file = np.random.choice(os.listdir(image_folder))
image_path = os.path.join(image_folder, random_image_file)
image_name = os.path.splitext(random_image_file)[0]
print("Image Path:", image_path)

# Load ground truth annotations
gt_object_detection_path = os.path.join(annotation_folder, f"{image_name}_bbox.npy")
gt_semantic_segmentation_path = os.path.join(annotation_folder, f"{image_name}_mask.npy")

gt_object_detection = np.load(gt_object_detection_path)
gt_semantic_segmentation = np.load(gt_semantic_segmentation_path)

# Apply object detection and semantic segmentation to the image
output_object_detection, object_detection_outputs = detect_objects(image_path)
output_semantic_segmentation, semantic_segmentation_outputs = segment_image(image_path)

# Calculate accuracy for object detection
pred_boxes = object_detection_outputs['instances'].pred_boxes.tensor.cpu().numpy()
precision, recall, f1 = calculate_object_detection_accuracy(pred_boxes, gt_object_detection)

# Calculate accuracy for semantic segmentation
pixel_accuracy, mean_iou = calculate_semantic_segmentation_accuracy(semantic_segmentation_outputs['sem_seg'], torch.tensor(gt_semantic_segmentation))

# Read the caption for the image
caption = read_caption(image_name)

# Display the input image, object detection, and semantic segmentation results along with the caption
fig, axs = plt.subplots(1, 3, figsize=(20, 10))
input_image = cv2.imread(image_path)[:, :, ::-1]
axs[0].imshow(input_image)
axs[0].set_title('Input Image')
axs[0].axis('off')
axs[1].imshow(output_object_detection)
axs[1].set_title(f'Object Detection\nPrecision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}')
axs[1].axis('off')
axs[2].imshow(output_semantic_segmentation)
axs[2].set_title(f'Semantic Segmentation\nPixel Acc: {pixel_accuracy:.2f}, Mean IoU: {mean_iou:.2f}')
axs[2].axis('off')

# Add caption below the images
plt.figtext(0.5, 0.01, caption, wrap=True, horizontalalignment='center', fontsize=12)

plt.show()

import os
import cv2
import numpy as np
import torch
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, jaccard_score
from PIL import Image
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
from detectron2.utils.visualizer import Visualizer
from detectron2.data.detection_utils import read_image
import matplotlib.pyplot as plt

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to the folders containing images and captions
image_folder = "/content/drive/My Drive/indian_vehicle_images"
caption_folder = "/content/drive/My Drive/indian_vehicle_txt_annos"
annotation_folder = "/content/drive/My Drive/indian_vehicle_annotations"  # Folder containing ground truth annotations

# Check if paths are correct
print("Images Path Exists:", os.path.exists(image_folder))
print("Captions Path Exists:", os.path.exists(caption_folder))
print("Annotations Path Exists:", os.path.exists(annotation_folder))

# Load the pre-trained Mask R-CNN model for object detection
print("Loading Mask R-CNN model for object detection...")
cfg_object_detection = get_cfg()
cfg_object_detection.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg_object_detection.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_object_detection.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
predictor_object_detection = DefaultPredictor(cfg_object_detection)
print("Mask R-CNN model for object detection loaded.")

# Load the pre-trained Mask R-CNN model for semantic segmentation
print("Loading Mask R-CNN model for semantic segmentation...")
cfg_semantic_segmentation = get_cfg()
cfg_semantic_segmentation.merge_from_file(model_zoo.get_config_file("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml"))
cfg_semantic_segmentation.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_semantic_segmentation.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml")
predictor_semantic_segmentation = DefaultPredictor(cfg_semantic_segmentation)
print("Mask R-CNN model for semantic segmentation loaded.")

# Function to apply object detection on image
def detect_objects(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_object_detection(im)
    v = Visualizer(im[:, :, ::-1], scale=1.2)
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    return out.get_image()[:, :, ::-1], outputs

# Function to apply semantic segmentation on image
def segment_image(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_semantic_segmentation(im)
    v = Visualizer(im[:, :, ::-1], metadata=predictor_semantic_segmentation.metadata, scale=1.2)
    out = v.draw_sem_seg(outputs["sem_seg"].argmax(dim=0).to("cpu"))
    return out.get_image()[:, :, ::-1], outputs

# Function to read caption from file
def read_caption(image_name):
    caption_file = os.path.join(caption_folder, f"{image_name}.txt")
    if os.path.exists(caption_file):
        with open(caption_file, 'r') as f:
            caption = f.read().strip()
        return caption
    else:
        return "No caption available."

# Function to calculate object detection accuracy
def calculate_object_detection_accuracy(preds, gt):
    iou_threshold = 0.5
    tp, fp, fn = 0, 0, 0

    for pred in preds:
        ious = [iou(pred, g) for g in gt]
        max_iou = max(ious) if ious else 0
        if max_iou >= iou_threshold:
            tp += 1
        else:
            fp += 1

    fn = len(gt) - tp

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    return precision, recall, f1

# Function to calculate IoU for bounding boxes
def iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])

    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)
    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)

    iou = interArea / float(boxAArea + boxBArea - interArea)
    return iou

# Function to calculate semantic segmentation accuracy
def calculate_semantic_segmentation_accuracy(pred, gt):
    pred = pred.argmax(dim=0).cpu().numpy()
    gt = gt.cpu().numpy()
    pixel_accuracy = np.mean(pred == gt)
    mean_iou = jaccard_score(gt.flatten(), pred.flatten(), average='macro')
    return pixel_accuracy, mean_iou

# Randomly select an image
random_image_file = np.random.choice(os.listdir(image_folder))
image_path = os.path.join(image_folder, random_image_file)
image_name = os.path.splitext(random_image_file)[0]
print("Image Path:", image_path)

# Placeholder for ground truth annotations (synthetic example)
def generate_synthetic_ground_truth(image_shape):
    h, w, _ = image_shape
    bbox = [w // 4, h // 4, 3 * w // 4, 3 * h // 4]  # Simple centered box
    mask = np.zeros((h, w), dtype=np.int32)
    mask[h // 4: 3 * h // 4, w // 4: 3 * w // 4] = 1  # Simple centered mask
    return np.array([bbox]), mask

# Check if ground truth annotations exist
gt_object_detection_path = os.path.join(annotation_folder, f"{image_name}_bbox.npy")
gt_semantic_segmentation_path = os.path.join(annotation_folder, f"{image_name}_mask.npy")

if os.path.exists(gt_object_detection_path) and os.path.exists(gt_semantic_segmentation_path):
    gt_object_detection = np.load(gt_object_detection_path)
    gt_semantic_segmentation = np.load(gt_semantic_segmentation_path)
else:
    # Generate synthetic ground truth annotations
    input_image = cv2.imread(image_path)
    gt_object_detection, gt_semantic_segmentation = generate_synthetic_ground_truth(input_image.shape)
    print("Ground truth annotations not found, using synthetic annotations.")

# Apply object detection and semantic segmentation to the image
output_object_detection, object_detection_outputs = detect_objects(image_path)
output_semantic_segmentation, semantic_segmentation_outputs = segment_image(image_path)

# Calculate accuracy for object detection
pred_boxes = object_detection_outputs['instances'].pred_boxes.tensor.cpu().numpy()
precision, recall, f1 = calculate_object_detection_accuracy(pred_boxes, gt_object_detection)

# Calculate accuracy for semantic segmentation
pixel_accuracy, mean_iou = calculate_semantic_segmentation_accuracy(semantic_segmentation_outputs['sem_seg'], torch.tensor(gt_semantic_segmentation))

# Read the caption for the image
caption = read_caption(image_name)

# Display the input image, object detection, and semantic segmentation results along with the caption
fig, axs = plt.subplots(1, 3, figsize=(20, 10))
input_image = cv2.imread(image_path)[:, :, ::-1]
axs[0].imshow(input_image)
axs[0].set_title('Input Image')
axs[0].axis('off')
axs[1].imshow(output_object_detection)
axs[1].set_title(f'Object Detection\nPrecision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}')
axs[1].axis('off')
axs[2].imshow(output_semantic_segmentation)
axs[2].set_title(f'Semantic Segmentation\nPixel Acc: {pixel_accuracy:.2f}, Mean IoU: {mean_iou:.2f}')
axs[2].axis('off')

# Add caption below the images
plt.figtext(0.5, 0.01, caption, wrap=True, horizontalalignment='center', fontsize=12)

plt.show()

import os
import cv2
import numpy as np
import torch
from sklearn.metrics import accuracy_score, jaccard_score
from PIL import Image
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
from detectron2.utils.visualizer import Visualizer
import matplotlib.pyplot as plt

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to the folders containing images and captions
image_folder = "/content/drive/My Drive/indian_vehicle_images"
caption_folder = "/content/drive/My Drive/indian_vehicle_txt_annos"
annotation_folder = "/content/drive/My Drive/indian_vehicle_annotations"  # Folder containing ground truth annotations

# Check if paths are correct
print("Images Path Exists:", os.path.exists(image_folder))
print("Captions Path Exists:", os.path.exists(caption_folder))
print("Annotations Path Exists:", os.path.exists(annotation_folder))

# Load the pre-trained Mask R-CNN model for object detection
print("Loading Mask R-CNN model for object detection...")
cfg_object_detection = get_cfg()
cfg_object_detection.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg_object_detection.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_object_detection.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
predictor_object_detection = DefaultPredictor(cfg_object_detection)
print("Mask R-CNN model for object detection loaded.")

# Load the pre-trained Mask R-CNN model for semantic segmentation
print("Loading Mask R-CNN model for semantic segmentation...")
cfg_semantic_segmentation = get_cfg()
cfg_semantic_segmentation.merge_from_file(model_zoo.get_config_file("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml"))
cfg_semantic_segmentation.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_semantic_segmentation.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml")
predictor_semantic_segmentation = DefaultPredictor(cfg_semantic_segmentation)
print("Mask R-CNN model for semantic segmentation loaded.")

# Function to apply object detection on image
def detect_objects(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_object_detection(im)
    v = Visualizer(im[:, :, ::-1], scale=1.2)
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    return out.get_image()[:, :, ::-1], outputs

# Function to apply semantic segmentation on image
def segment_image(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_semantic_segmentation(im)
    v = Visualizer(im[:, :, ::-1], metadata=predictor_semantic_segmentation.metadata, scale=1.2)
    out = v.draw_sem_seg(outputs["sem_seg"].argmax(dim=0).to("cpu"))
    return out.get_image()[:, :, ::-1], outputs

# Function to read caption from file
def read_caption(image_name):
    caption_file = os.path.join(caption_folder, f"{image_name}.txt")
    if os.path.exists(caption_file):
        with open(caption_file, 'r') as f:
            caption = f.read().strip()
        return caption
    else:
        return "No caption available."

# Function to calculate object detection accuracy
def calculate_object_detection_accuracy(preds, gt):
    iou_threshold = 0.5
    tp, fp, fn = 0, 0, 0

    for pred in preds:
        ious = [iou(pred, g) for g in gt]
        max_iou = max(ious) if ious else 0
        if max_iou >= iou_threshold:
            tp += 1
        else:
            fp += 1

    fn = len(gt) - tp

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    return precision, recall, f1

# Function to calculate IoU for bounding boxes
def iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])

    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)
    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)

    iou = interArea / float(boxAArea + boxBArea - interArea)
    return iou

# Function to calculate semantic segmentation accuracy
def calculate_semantic_segmentation_accuracy(pred, gt):
    pred = pred.argmax(dim=0).cpu().numpy()
    gt = gt.cpu().numpy()
    pixel_accuracy = np.mean(pred == gt)
    mean_iou = jaccard_score(gt.flatten(), pred.flatten(), average='macro')
    return pixel_accuracy, mean_iou

# Randomly select an image
random_image_file = np.random.choice(os.listdir(image_folder))
image_path = os.path.join(image_folder, random_image_file)
image_name = os.path.splitext(random_image_file)[0]
print("Image Path:", image_path)

# Placeholder for ground truth annotations (synthetic example)
def generate_synthetic_ground_truth(image_shape):
    h, w, _ = image_shape
    bbox = [w // 4, h // 4, 3 * w // 4, 3 * h // 4]  # Simple centered box
    mask = np.zeros((h, w), dtype=np.int32)
    mask[h // 4: 3 * h // 4, w // 4: 3 * w // 4] = 1  # Simple centered mask
    return np.array([bbox]), mask

# Check if ground truth annotations exist
gt_object_detection_path = os.path.join(annotation_folder, f"{image_name}_bbox.npy")
gt_semantic_segmentation_path = os.path.join(annotation_folder, f"{image_name}_mask.npy")

if os.path.exists(gt_object_detection_path) and os.path.exists(gt_semantic_segmentation_path):
    gt_object_detection = np.load(gt_object_detection_path)
    gt_semantic_segmentation = np.load(gt_semantic_segmentation_path)
else:
    # Generate synthetic ground truth annotations
    input_image = cv2.imread(image_path)
    gt_object_detection, gt_semantic_segmentation = generate_synthetic_ground_truth(input_image.shape)
    print("Ground truth annotations not found, using synthetic annotations.")

# Apply object detection and semantic segmentation to the image
output_object_detection, object_detection_outputs = detect_objects(image_path)
output_semantic_segmentation, semantic_segmentation_outputs = segment_image(image_path)

# Calculate accuracy for object detection
pred_boxes = object_detection_outputs['instances'].pred_boxes.tensor.cpu().numpy()
precision, recall, f1 = calculate_object_detection_accuracy(pred_boxes, gt_object_detection)

# Calculate accuracy for semantic segmentation
pred_segmentation = semantic_segmentation_outputs['sem_seg']
gt_segmentation = torch.tensor(gt_semantic_segmentation)
pixel_accuracy, mean_iou = calculate_semantic_segmentation_accuracy(pred_segmentation, gt_segmentation)

# Read the caption for the image
caption = read_caption(image_name)

# Display the input image, object detection, and semantic segmentation results along with the caption
fig, axs = plt.subplots(1, 4, figsize=(20, 10))
input_image = cv2.imread(image_path)[:, :, ::-1]
axs[0].imshow(input_image)
axs[0].set_title('Input Image')
axs[0].axis('off')
axs[1].imshow(output_object_detection)
axs[1].set_title(f'Object Detection\nPrecision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}')
axs[1].axis('off')
axs[2].imshow(output_semantic_segmentation)
axs[2].set_title(f'Semantic Segmentation\nPixel Acc: {pixel_accuracy:.2f}, Mean IoU: {mean_iou:.2f}')
axs[2].axis('off')

# Visualize synthetic ground truth for verification
gt_image = input_image.copy()
for bbox in gt_object_detection:
    cv2.rectangle(gt_image, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 0, 0), 2)
gt_mask_image = np.zeros_like(input_image)
gt_mask_image[gt_semantic_segmentation == 1] = [0, 255, 0]
axs[3].imshow(gt_image)
axs[3].imshow(gt_mask_image, alpha=0.5)
axs[3].set_title('Ground Truth\n(Blue: bbox, Green: mask)')
axs[3].axis('off')

# Add caption below the images
plt.figtext(0.5, 0.01, caption, wrap=True, horizontalalignment='center', fontsize=12)

plt.show()

import os
import cv2
import numpy as np
import torch
from sklearn.metrics import accuracy_score, jaccard_score
from PIL import Image
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
from detectron2.utils.visualizer import Visualizer
import matplotlib.pyplot as plt

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to the folders containing images and captions
image_folder = "/content/drive/My Drive/indian_vehicle_images"
caption_folder = "/content/drive/My Drive/indian_vehicle_txt_annos"
annotation_folder = "/content/drive/My Drive/indian_vehicle_annotations"  # Folder containing ground truth annotations

# Check if paths are correct
print("Images Path Exists:", os.path.exists(image_folder))
print("Captions Path Exists:", os.path.exists(caption_folder))
print("Annotations Path Exists:", os.path.exists(annotation_folder))

# Load the pre-trained Mask R-CNN model for object detection
print("Loading Mask R-CNN model for object detection...")
cfg_object_detection = get_cfg()
cfg_object_detection.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg_object_detection.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_object_detection.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
predictor_object_detection = DefaultPredictor(cfg_object_detection)
print("Mask R-CNN model for object detection loaded.")

# Load the pre-trained Mask R-CNN model for semantic segmentation
print("Loading Mask R-CNN model for semantic segmentation...")
cfg_semantic_segmentation = get_cfg()
cfg_semantic_segmentation.merge_from_file(model_zoo.get_config_file("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml"))
cfg_semantic_segmentation.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_semantic_segmentation.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml")
predictor_semantic_segmentation = DefaultPredictor(cfg_semantic_segmentation)
print("Mask R-CNN model for semantic segmentation loaded.")

# Function to apply object detection on image
def detect_objects(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_object_detection(im)
    v = Visualizer(im[:, :, ::-1], scale=1.2)
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    return out.get_image()[:, :, ::-1], outputs

# Function to apply semantic segmentation on image
def segment_image(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_semantic_segmentation(im)
    v = Visualizer(im[:, :, ::-1], metadata=predictor_semantic_segmentation.metadata, scale=1.2)
    out = v.draw_sem_seg(outputs["sem_seg"].argmax(dim=0).to("cpu"))
    return out.get_image()[:, :, ::-1], outputs

# Function to read caption from file
def read_caption(image_name):
    caption_file = os.path.join(caption_folder, f"{image_name}.txt")
    if os.path.exists(caption_file):
        with open(caption_file, 'r') as f:
            caption = f.read().strip()
        return caption
    else:
        return "No caption available."

# Function to calculate object detection accuracy
def calculate_object_detection_accuracy(preds, gt):
    iou_threshold = 0.5
    tp, fp, fn = 0, 0, 0

    for pred in preds:
        ious = [iou(pred, g) for g in gt]
        max_iou = max(ious) if ious else 0
        if max_iou >= iou_threshold:
            tp += 1
        else:
            fp += 1

    fn = len(gt) - tp

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    return precision, recall, f1

# Function to calculate IoU for bounding boxes
def iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])

    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)
    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)

    iou = interArea / float(boxAArea + boxBArea - interArea)
    return iou

# Function to calculate semantic segmentation accuracy
def calculate_semantic_segmentation_accuracy(pred, gt):
    pred = pred.argmax(dim=0).cpu().numpy()
    gt = gt.cpu().numpy()
    pixel_accuracy = np.mean(pred == gt)
    mean_iou = jaccard_score(gt.flatten(), pred.flatten(), average='macro')
    return pixel_accuracy, mean_iou

# Randomly select an image
random_image_file = np.random.choice(os.listdir(image_folder))
image_path = os.path.join(image_folder, random_image_file)
image_name = os.path.splitext(random_image_file)[0]
print("Image Path:", image_path)

# Placeholder for ground truth annotations (synthetic example)
def generate_synthetic_ground_truth(image_shape):
    h, w, _ = image_shape
    bbox = [w // 4, h // 4, 3 * w // 4, 3 * h // 4]  # Simple centered box
    mask = np.zeros((h, w), dtype=np.int32)
    mask[h // 4: 3 * h // 4, w // 4: 3 * w // 4] = 1  # Simple centered mask
    return np.array([bbox]), mask

# Check if ground truth annotations exist
gt_object_detection_path = os.path.join(annotation_folder, f"{image_name}_bbox.npy")
gt_semantic_segmentation_path = os.path.join(annotation_folder, f"{image_name}_mask.npy")

if os.path.exists(gt_object_detection_path) and os.path.exists(gt_semantic_segmentation_path):
    gt_object_detection = np.load(gt_object_detection_path)
    gt_semantic_segmentation = np.load(gt_semantic_segmentation_path)
else:
    # Generate synthetic ground truth annotations
    input_image = cv2.imread(image_path)
    gt_object_detection, gt_semantic_segmentation = generate_synthetic_ground_truth(input_image.shape)
    print("Ground truth annotations not found, using synthetic annotations.")

# Apply object detection and semantic segmentation to the image
output_object_detection, object_detection_outputs = detect_objects(image_path)
output_semantic_segmentation, semantic_segmentation_outputs = segment_image(image_path)

# Calculate accuracy for object detection
pred_boxes = object_detection_outputs['instances'].pred_boxes.tensor.cpu().numpy()
print("Predicted boxes:", pred_boxes)
print("Ground truth boxes:", gt_object_detection)
precision, recall, f1 = calculate_object_detection_accuracy(pred_boxes, gt_object_detection)
print(f"Precision: {precision}, Recall: {recall}, F1-score: {f1}")

# Calculate accuracy for semantic segmentation
pred_segmentation = semantic_segmentation_outputs['sem_seg']
gt_segmentation = torch.tensor(gt_semantic_segmentation)
pixel_accuracy, mean_iou = calculate_semantic_segmentation_accuracy(pred_segmentation, gt_segmentation)
print(f"Pixel Accuracy: {pixel_accuracy}, Mean IoU: {mean_iou}")

# Read the caption for the image
caption = read_caption(image_name)

# Display the input image, object detection, and semantic segmentation results along with the caption
fig, axs = plt.subplots(1, 4, figsize=(20, 10))
input_image = cv2.imread(image_path)[:, :, ::-1]
axs[0].imshow(input_image)
axs[0].set_title('Input Image')
axs[0].axis('off')
axs[1].imshow(output_object_detection)
axs[1].set_title(f'Object Detection\nPrecision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}')
axs[1].axis('off')
axs[2].imshow(output_semantic_segmentation)
axs[2].set_title(f'Semantic Segmentation\nPixel Acc: {pixel_accuracy:.2f}, Mean IoU: {mean_iou:.2f}')
axs[2].axis('off')

# Visualize synthetic ground truth for verification
gt_image = input_image.copy()
for bbox in gt_object_detection:
    cv2.rectangle(gt_image, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 0, 0), 2)
gt_mask_image = np.zeros_like(input_image)
gt_mask_image[gt_semantic_segmentation == 1] = [0, 255, 0]
axs[3].imshow(gt_image)
axs[3].imshow(gt_mask_image, alpha=0.5)
axs[3].set_title('Ground Truth\n(Blue: bbox, Green: mask)')
axs[3].axis('off')

# Add caption below the images
plt.figtext(0.5, 0.01, caption, wrap=True, horizontalalignment='center', fontsize=12)

plt.show()



import os
import cv2
import numpy as np
import torch
import torchvision.transforms as T
from PIL import Image
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
from detectron2.utils.visualizer import Visualizer
import matplotlib.pyplot as plt
import spacy

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to the folders containing images and captions
image_folder = "/content/drive/My Drive/indian_vehicle_images"
caption_folder = "/content/drive/My Drive/indian_vehicle_txt_annos"

# Check if paths are correct
print("Images Path Exists:", os.path.exists(image_folder))
print("Captions Path Exists:", os.path.exists(caption_folder))

# Load the pre-trained Mask R-CNN model for object detection
print("Loading Mask R-CNN model for object detection...")
cfg_object_detection = get_cfg()
cfg_object_detection.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg_object_detection.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_object_detection.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
predictor_object_detection = DefaultPredictor(cfg_object_detection)
print("Mask R-CNN model for object detection loaded.")

# Load the pre-trained Mask R-CNN model for semantic segmentation
print("Loading Mask R-CNN model for semantic segmentation...")
cfg_semantic_segmentation = get_cfg()
cfg_semantic_segmentation.merge_from_file(model_zoo.get_config_file("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml"))
cfg_semantic_segmentation.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_semantic_segmentation.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml")
predictor_semantic_segmentation = DefaultPredictor(cfg_semantic_segmentation)
print("Mask R-CNN model for semantic segmentation loaded.")

# Load English tokenizer, tagger, parser, NER, and word vectors
nlp = spacy.load("en_core_web_sm")

# Function to parse text annotations
def parse_annotations(image_name):
    annotation_file = os.path.join(caption_folder, f"{image_name}.txt")
    annotations = []
    if os.path.exists(annotation_file):
        with open(annotation_file, 'r') as f:
            for line in f:
                # Parse the text description using spaCy
                doc = nlp(line.strip())
                # Extract relevant entities (e.g., object names) from the description
                entities = [ent.text.lower() for ent in doc.ents]
                # Assuming each entity is a class label, create annotations accordingly
                for entity in entities:
                    annotations.append({"class_label": entity})
    return annotations

# Function to calculate accuracy
def calculate_accuracy(predictions, ground_truth):
    num_correct = sum(1 for pred in predictions if pred in ground_truth)
    return num_correct / len(predictions) if len(predictions) > 0 else 0.0

# Randomly select an image
random_image_file = np.random.choice(os.listdir(image_folder))
image_path = os.path.join(image_folder, random_image_file)
image_name = os.path.splitext(random_image_file)[0]
print("Image Path:", image_path)

# Parse annotations for the selected image
gt_annotations = parse_annotations(image_name)

# Apply object detection and semantic segmentation to the image
output_object_detection, object_detection_outputs = detect_objects(image_path)
output_semantic_segmentation, semantic_segmentation_outputs = segment_image(image_path)

# Get predictions from object detection and semantic segmentation outputs
object_detection_predictions = [(i, cfg_object_detection.MODEL.ROI_HEADS.SCORE_THRESH_TEST) for i in object_detection_outputs["instances"].to("cpu").pred_classes.tolist()]

semantic_segmentation_predictions = [label.item() for label in semantic_segmentation_outputs["sem_seg"].argmax(dim=0).to("cpu").flatten().tolist()]

# Calculate accuracy
object_detection_accuracy = calculate_accuracy(object_detection_predictions, gt_annotations)
semantic_segmentation_accuracy = calculate_accuracy(semantic_segmentation_predictions, gt_annotations)

# Display the input image, object detection, and semantic segmentation results
fig, axs = plt.subplots(1, 3, figsize=(15, 5))
input_image = cv2.imread(image_path)[:, :, ::-1]
axs[0].imshow(input_image)
axs[0].set_title('Input Image')
axs[0].axis('off')
axs[1].imshow(output_object_detection)
axs[1].set_title(f'Object Detection\nAccuracy: {object_detection_accuracy:.2f}')
axs[1].axis('off')
axs[2].imshow(output_semantic_segmentation)
axs[2].set_title(f'Semantic Segmentation\nAccuracy: {semantic_segmentation_accuracy:.2f}')
axs[2].axis('off')

plt.show()

import os
import cv2
import numpy as np
import torch
from sklearn.metrics import accuracy_score, jaccard_score
from PIL import Image
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
from detectron2.utils.visualizer import Visualizer
import matplotlib.pyplot as plt

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to the folders containing images and captions
image_folder = "/content/drive/My Drive/indian_vehicle_images"
caption_folder = "/content/drive/My Drive/indian_vehicle_txt_annos"
annotation_folder = "/content/drive/My Drive/indian_vehicle_annotations"  # Folder containing ground truth annotations

# Check if paths are correct
print("Images Path Exists:", os.path.exists(image_folder))
print("Captions Path Exists:", os.path.exists(caption_folder))
print("Annotations Path Exists:", os.path.exists(annotation_folder))

# Load the pre-trained Mask R-CNN model for object detection
print("Loading Mask R-CNN model for object detection...")
cfg_object_detection = get_cfg()
cfg_object_detection.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg_object_detection.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_object_detection.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
predictor_object_detection = DefaultPredictor(cfg_object_detection)
print("Mask R-CNN model for object detection loaded.")

# Load the pre-trained Mask R-CNN model for semantic segmentation
print("Loading Mask R-CNN model for semantic segmentation...")
cfg_semantic_segmentation = get_cfg()
cfg_semantic_segmentation.merge_from_file(model_zoo.get_config_file("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml"))
cfg_semantic_segmentation.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_semantic_segmentation.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml")
predictor_semantic_segmentation = DefaultPredictor(cfg_semantic_segmentation)
print("Mask R-CNN model for semantic segmentation loaded.")

# Function to apply object detection on image
def detect_objects(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_object_detection(im)
    v = Visualizer(im[:, :, ::-1], scale=1.2)
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    return out.get_image()[:, :, ::-1], outputs

# Function to apply semantic segmentation on image
def segment_image(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_semantic_segmentation(im)
    v = Visualizer(im[:, :, ::-1], metadata=predictor_semantic_segmentation.metadata, scale=1.2)
    out = v.draw_sem_seg(outputs["sem_seg"].argmax(dim=0).to("cpu"))
    return out.get_image()[:, :, ::-1], outputs

# Function to read caption from file
def read_caption(image_name):
    caption_file = os.path.join(caption_folder, f"{image_name}.txt")
    if os.path.exists(caption_file):
        with open(caption_file, 'r') as f:
            caption = f.read().strip()
        return caption
    else:
        return "No caption available."

# Function to calculate object detection accuracy
def calculate_object_detection_accuracy(preds, gt):
    iou_threshold = 0.5
    tp, fp, fn = 0, 0, 0

    for pred in preds:
        ious = [iou(pred, g) for g in gt]
        max_iou = max(ious) if ious else 0
        if max_iou >= iou_threshold:
            tp += 1
        else:
            fp += 1

    fn = len(gt) - tp

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    return precision, recall, f1

# Function to calculate IoU for bounding boxes
def iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])

    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)
    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)

    iou = interArea / float(boxAArea + boxBArea - interArea)
    return iou

# Function to calculate semantic segmentation accuracy
def calculate_semantic_segmentation_accuracy(pred, gt):
    pred = pred.argmax(dim=0).cpu().numpy()
    gt = gt.cpu().numpy()
    pixel_accuracy = np.mean(pred == gt)
    mean_iou = jaccard_score(gt.flatten(), pred.flatten(), average='macro')
    return pixel_accuracy, mean_iou

# Randomly select an image
random_image_file = np.random.choice(os.listdir(image_folder))
image_path = os.path.join(image_folder, random_image_file)
image_name = os.path.splitext(random_image_file)[0]
print("Image Path:", image_path)

# Placeholder for ground truth annotations (synthetic example)
def generate_synthetic_ground_truth(image_shape):
    h, w, _ = image_shape
    bbox = [w // 4, h // 4, 3 * w // 4, 3 * h // 4]  # Simple centered box
    mask = np.zeros((h, w), dtype=np.int32)
    mask[h // 4: 3 * h // 4, w // 4: 3 * w // 4] = 1  # Simple centered mask
    return np.array([bbox]), mask

# Check if ground truth annotations exist
gt_object_detection_path = os.path.join(annotation_folder, f"{image_name}_bbox.npy")
gt_semantic_segmentation_path = os.path.join(annotation_folder, f"{image_name}_mask.npy")

if os.path.exists(gt_object_detection_path) and os.path.exists(gt_semantic_segmentation_path):
    gt_object_detection = np.load(gt_object_detection_path)
    gt_semantic_segmentation = np.load(gt_semantic_segmentation_path)
else:
    # Generate synthetic ground truth annotations
    input_image = cv2.imread(image_path)
    gt_object_detection, gt_semantic_segmentation = generate_synthetic_ground_truth(input_image.shape)
    print("Ground truth annotations not found, using synthetic annotations.")

# Apply object detection and semantic segmentation to the image
output_object_detection, object_detection_outputs = detect_objects(image_path)
output_semantic_segmentation, semantic_segmentation_outputs = segment_image(image_path)

# Calculate accuracy for object detection
pred_boxes = object_detection_outputs['instances'].pred_boxes.tensor.cpu().numpy()
print("Predicted boxes:", pred_boxes)
print("Ground truth boxes:", gt_object_detection)
precision, recall, f1 = calculate_object_detection_accuracy(pred_boxes, gt_object_detection)
print(f"Precision: {precision}, Recall: {recall}, F1-score: {f1}")

# Calculate accuracy for semantic segmentation
pred_segmentation = semantic_segmentation_outputs['sem_seg']
gt_segmentation = torch.tensor(gt_semantic_segmentation)
pixel_accuracy, mean_iou = calculate_semantic_segmentation_accuracy(pred_segmentation, gt_segmentation)
print(f"Pixel Accuracy: {pixel_accuracy}, Mean IoU: {mean_iou}")

# Read the caption for the image
caption = read_caption(image_name)

# Display the input image, object detection, and semantic segmentation results along with the caption
fig, axs = plt.subplots(1, 4, figsize=(20, 10))
input_image = cv2.imread(image_path)[:, :, ::-1]
axs[0].imshow(input_image)
axs[0].set_title('Input Image')
axs[0].axis('off')
axs[1].imshow(output_object_detection)
axs[1].set_title(f'Object Detection\nPrecision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}')
axs[1].axis('off')
axs[2].imshow(output_semantic_segmentation)
axs[2].set_title(f'Semantic Segmentation\nPixel Acc: {pixel_accuracy:.2f}, Mean IoU: {mean_iou:.2f}')
axs[2].axis('off')

# Visualize synthetic ground truth for verification
gt_image = input_image.copy()
for bbox in gt_object_detection:
    cv2.rectangle(gt_image, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 0, 0), 2)
gt_mask_image = np.zeros_like(input_image)
gt_mask_image[gt_semantic_segmentation == 1] = [0, 255, 0]
axs[3].imshow(gt_image)
axs[3].imshow(gt_mask_image, alpha=0.5)
axs[3].set_title('Ground Truth\n(Blue: bbox, Green: mask)')
axs[3].axis('off')

# Add caption below the images
plt.figtext(0.5, 0.01, caption, wrap=True, horizontalalignment='center', fontsize=12)

plt.show()

import os
import cv2
import numpy as np
import torch
import torchvision.transforms as T
from PIL import Image
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
from detectron2.utils.visualizer import Visualizer
import matplotlib.pyplot as plt

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to the folders containing images and captions
image_folder = "/content/drive/My Drive/indian_vehicle_images"
caption_folder = "/content/drive/My Drive/indian_vehicle_txt_annos"

# Check if paths are correct
print("Images Path Exists:", os.path.exists(image_folder))
print("Captions Path Exists:", os.path.exists(caption_folder))

# Load the pre-trained Mask R-CNN model for object detection
print("Loading Mask R-CNN model for object detection...")
cfg_object_detection = get_cfg()
cfg_object_detection.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg_object_detection.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_object_detection.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
predictor_object_detection = DefaultPredictor(cfg_object_detection)
print("Mask R-CNN model for object detection loaded.")

# Load the pre-trained Mask R-CNN model for semantic segmentation
print("Loading Mask R-CNN model for semantic segmentation...")
cfg_semantic_segmentation = get_cfg()
cfg_semantic_segmentation.merge_from_file(model_zoo.get_config_file("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml"))
cfg_semantic_segmentation.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
cfg_semantic_segmentation.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml")
predictor_semantic_segmentation = DefaultPredictor(cfg_semantic_segmentation)
print("Mask R-CNN model for semantic segmentation loaded.")

# Function to apply object detection on image
def detect_objects(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_object_detection(im)
    v = Visualizer(im[:, :, ::-1], scale=1.2)
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    return out.get_image()[:, :, ::-1]

# Function to apply semantic segmentation on image
def segment_image(image_path):
    im = cv2.imread(image_path)
    outputs = predictor_semantic_segmentation(im)
    v = Visualizer(im[:, :, ::-1], metadata=predictor_semantic_segmentation.metadata, scale=1.2)
    out = v.draw_sem_seg(outputs["sem_seg"].argmax(dim=0).to("cpu"))
    return out.get_image()[:, :, ::-1]

# Function to read caption from file
def read_caption(image_name):
    caption_file = os.path.join(caption_folder, f"{image_name}.txt")
    if os.path.exists(caption_file):
        with open(caption_file, 'r') as f:
            caption = f.read().strip()
        return caption
    else:
        return "No caption available."

# Randomly select an image
random_image_file = np.random.choice(os.listdir(image_folder))
image_path = os.path.join(image_folder, random_image_file)
image_name = os.path.splitext(random_image_file)[0]
print("Image Path:", image_path)

# Apply object detection and semantic segmentation to the image
output_object_detection = detect_objects(image_path)
output_semantic_segmentation = segment_image(image_path)

# Read the caption for the image
caption = read_caption(image_name)

# Display the input image, object detection, and semantic segmentation results along with the caption
fig, axs = plt.subplots(1, 3, figsize=(15, 5))
input_image = cv2.imread(image_path)[:, :, ::-1]
axs[0].imshow(input_image)
axs[0].set_title('Input Image')
axs[0].axis('off')
axs[2].imshow(output_object_detection)
axs[2].set_title('Object Detection')
axs[2].axis('off')
axs[1].imshow(output_semantic_segmentation)
axs[1].set_title('Semantic Segmentation')
axs[1].axis('off')

# Add caption below the images
plt.figtext(0.5, 0.01, caption, wrap=True, horizontalalignment='center', fontsize=12)

plt.show()